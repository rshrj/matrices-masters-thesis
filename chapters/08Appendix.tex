\chapter{Numerical Methods}
\label{app:numerical-methods}

The initial data of the BFSS system are $9$ $N \times N$ Hermitian matrices $X^{1}(0), \ldots, X^{K}(0)$ and their velocities $\dot{X}^{1}(0), \ldots, \dot{X}^{K}(0)$. Generating a random $N \times N$ Hermitian matrix is easy; generate $N^2$ random complex numbers using something like \texttt{RandomComplex} in Mathematica and assemble them into a complex $N \times N$ matrix $Z$ and take the Hermitian part $\frac{1}{2}(Z + Z^{\dagger})$.

The trace of the various matrices is unimportant because of "translation" symmetry $X^{k} \to X^{k} + \epsilon^{k} \boldsymbol{1}_N$ generated by the total momentum $\Tr \dot{X}^{k}$ which we set equal to zero, along with the center of mass position $\Tr X^{k}$, effectively working in the "center of mass" frame. We can thus remove traces of the various matrices above by $Z \to Z - \frac{\Tr Z}{N} \boldsymbol{1}_N $.

Of course, the problem with generating initial states like this is that they will generically be on completely disjoint phase trajectories. In order to study thermalization or some universal behavior, it is necessary to start with states of fixed macroscopic charges, which arise from the following symmetries.

\begin{enumerate}
  \item \textbf{Time translation} $\quad t \to t + \epsilon \quad \delta X^{k} = -\epsilon \dot{X}^{k}$
    \begin{equation}
      E = \Tr \left( \frac{1}{2} \dot{X}^{k} \dot{X}^{k} - \frac{1}{4} \comm{X^{i}}{X^{j}} \comm{X^{i}}{X^{j}} \right)
    \end{equation}
  \item \textbf{$\mathrm{SO}(K)$ Rotations} $\quad \omega_{k j} = -\omega_{j k} \quad \delta X^{k} = \omega_{k j} X^{j}$
    \begin{equation}
      J^{i j} = \Tr \left( X^{i} \dot{X}^{j} - X^{j} \dot{X}^{i} \right)
    \end{equation}
  \item \textbf{$\mathrm{SU}(N)$ Gauge transformations} $\quad M^{\dagger} = -M \quad \delta X^{k} = \comm{M}{X^{k}}$
    \begin{equation}
      G = \comm{X^{k}}{\dot{X}^{k}}
    \end{equation}
    To study the gauged system, $G$ must be set to zero.
\end{enumerate}

In total, there are $1 + \frac{K(K-1)}{2} + (N^2 - 1) + K = N^2 + \frac{K(K+1)}{2}$ real charges\footnote{The last term is the translation symmetry charge, the total momentum, which we set to zero}. 

The simplest way to fix the charges and satisfy the Gauge constraint is to set all the initial velocities to zero
\begin{equation}
  \dot{X}^i(0) = 0
\end{equation}
and we do exactly this.

It is very useful to rewrite the matrix variables in a convenient basis of the lie algebra $\mathfrak{su}(N)$ of $N \times N$ traceless Hermitian matrices. We use a generalized Gell-Mann type basis given by $\frac{N(N-1)}{2}$ real symmetric matrices, $\frac{N(N-1)}{2}$ imaginary antisymmetric matrices and $N-1$ real diagonal matrices (which also generate the Cartan subalgebra in this basis).
\begin{equation}\label{eqn:suN-basis}
  \begin{gathered}
    \underbrace{
      \frac{1}{2}\begin{pmatrix}
      & & & \\
      & & 1 & \\
      & 1 & & \\
      & & &
    \end{pmatrix}}_{\text{real symmetric}} \qquad%
    \underbrace{
      \frac{1}{2}\begin{pmatrix}
      & & & \\
      & & -i & \\
      & i & & \\
      & & &
    \end{pmatrix}}_{\text{imaginary antisymmetric}} \\
    \underbrace{
      \frac{1}{\sqrt{2r(r-1)}}\begin{pmatrix}
      1 & & & & \\
      & \ddots & & & \\
      & & 1 & & \\
      & & & 1-r & \\
      & & & &
    \end{pmatrix}}_{\text{real diagonal}}
  \end{gathered}
\end{equation}
These are normalized so that
\begin{equation}\label{eqn:cartan-killing}
\Tr t^{a} t^{b} = \frac{1}{2} \delta^{a b}
\end{equation}
We chose the physicist's convention for the Lie algebra of $\mathrm{SU}(N)$, meaning that the fundamental commutation relations of the $t^a$s contain a factor of $i$. They are
\begin{equation}
\comm{t^a}{t^b} = i f^{a b c} t^{c}
\end{equation}

It is easy to compute the structure constants $f^{a b c}$ in the basis we use \cref{eqn:suN-basis}. Due to the diagonal Cartan-Killing form \cref{eqn:cartan-killing}, the structure constants are completely antisymmetric in this basis. We can now expand all our matrix degrees of freedom in this basis
\begin{equation}
X^i(t) = x^{i}_{a}(t) t^a
\end{equation}
the matrix model equations of motion \cref{eqn:eom} can then be written as
\begin{equation}
\ddot{x}^i_a = f^{a e d} f^{e b c} x^i_b x^j_c x^j_d
\end{equation}

As stated in Chapter 3, the initial "positions" $x^i_a(0)$ are taken to be random. In particular, the procedure we follow to generate them is
\begin{enumerate}
  \item[a.] Generate $\tilde{X}^{i}_{a}$ as $K(N^2-1)$ random real numbers uniformly distributed in $(-1, 1)$.
  \item[b.] {
    Compute the energy $E$ of this state (with no velocity contribution) as%
    \begin{equation}
      E = \frac{1}{8} f^{a b e} f^{c d e} \tilde{X}^i_a \tilde{X}^j_b \tilde{X}^i_c \tilde{X}^j_d
    \end{equation}
  }
  \item[c.] {
    The random initial state we study $X^i_a$ is then given by
    \begin{equation}
      X^i_a = \lambda \tilde{X}^i_a \qquad \lambda = E^{-1/4}
    \end{equation}
  }
\end{enumerate}
By construction, the state just generated has energy $E = 1$. We now pose this as an initial value problem (IVP) 
\begin{align}
  \dot{x}^i_a &= p^i_a \qquad x^i_a(0) = X^i_a \\
  \dot{p}^i_a &= f^{a e d} f^{e b c} x^i_b x^j_c x^j_d \qquad p^i_a(0) = 0
\end{align}

Initial value problems (IVPs) are abundant in all of the sciences. There are numerous numerical methods and packages that can be used to solve IVPs of different types. The type of system we have, is, of course, a symplectic IVP since it is a Hamiltonian system. A standard numerical algorithm for solving symplectic systems is the Runge-Kutta-Nystroem family. We use the symplectic McLachlan symmetric B3A stepper implementing sixth order RKN \cite{doi:10.1137/0916010}. 

As a platform for our simulations, we chose to write C++ programs to optimize computational resources as much as possible. The Boost C++ library \cite{sch√§lingboost} comes very handy in this regard as it already contains a robust implementation of symplectic RKN methods. Our simulation was run on the IITM Quantum cluster with parallel programming optimizations. An example of runtimes involved is: at $N = 9, K = 9$, and for a length of time $T = 1000$ and step size $dt = 0.1$, it takes around 5 hours on an Intel(R) Xeon(R) Gold 5220R CPU @ 2.20GHz on a single core.



\chapter{Gaussian Unitary Ensemble}
\label{app:gue}

Perhaps the simplest distribution for an $N \times N$ complex matrix $Y$ is one in which the real and imaginary parts of every matrix entry are distributed (independently and identically) according to a normal distribution. In particular, if
\begin{equation}
  Y = (y_{i j})_{i, j = 1}^{N}
\end{equation}
then the distribution would be
\begin{equation}\label{eqn:complex-mat-dist}
  \begin{aligned}
    p(y_{i j}) &= \prod_{i j} \frac{1}{2\pi \sigma^2} \exp \left( -\frac{1}{2 \sigma^2} y_{i j} \overline{y}_{i j} \right) \\
    &= \left( \frac{1}{2\pi \sigma^2} \right)^{N^2} \exp \left( -\frac{1}{2 \sigma^2} \sum_{i j} y_{i j} \overline{y}_{i j} \right) \\
    &= \frac{1}{Z} \exp \left( -\frac{1}{2 \sigma^2} \Tr Y^\dagger Y \right)
  \end{aligned}
\end{equation}
with
\begin{equation}
  \log Z = N^2 \log(2 \pi \sigma^2)
\end{equation}
being the partition function used for normalization.

However, we are interested in traceless Hermitian matrices. But once one can generate random "normal" complex matrices $Y$, it is elementary to transform them into traceless Hermitian matrices. We do this in two steps
\begin{enumerate}
  \item[a.] {
    Pick out the Hermitian part $\tilde{X}$
    \begin{equation}
      Y \to \tilde{X} = \frac{Y + Y^{\dagger}}{2}
    \end{equation}
  }
  \item[b.] {
    Remove the trace
    \begin{equation}
      \tilde{X} \to X = \tilde{X} - \frac{Tr\tilde{X}}{N} \mathbf{1}_N
    \end{equation}
  }
\end{enumerate}

The distribution of the random matrix variable $\tilde{X}$ has a distribution closely related to \cref{eqn:complex-mat-dist}, namely
\begin{equation}
  p(\tilde{X}) = \frac{1}{Z_{\mathrm{GUE}}} \exp \left( -\frac{1}{2 \sigma^2} \Tr \tilde{X}^2 \right)
\end{equation}
This is called the Gaussian Unitary Ensemble (GUE). Although the functional form of the distribution looks very much like \cref{eqn:complex-mat-dist}, the partition function $Z_{\mathrm{GUE}}$ has changed since the probability measure now contains only half as many integration variables as before ($N^2$ now, $2N^2$ for $Y$).

Finally, removing the trace results in the so-called traceless GUE with, again, the same functional form for the distribution but a different partition function we call $Z_\mathrm{tGUE}$.

It is interesting to look at the distribution of eigenvalues $\left( \lambda_1, \ldots, \lambda_N \right)$ resulting from, say, GUE. The joint distribution (for the case $\sigma = 1$) turns out to be
\begin{equation}
  \rho(\lambda_1, \ldots, \lambda_N) = \frac{1}{\mathcal{Z}} e^{-\frac{1}{2} \sum_{i = 1}^{N} \lambda_i^2} \prod_{i < j} (\lambda_i - \lambda_j)^2
\end{equation}
where $\mathcal{Z}$ is given by a kind of Mehta's integral
\begin{equation}
  \mathcal{Z} = (2 \pi)^{N/2} \prod_{k = 1}^{N} k!
\end{equation}

One can integrate out all but one of the eigenvalues to obtain the marginal distribution of one of the eigenvalues. It can be shown that it is the same distribution as what one would obtain if one were to combine all the eigenvalues and treat them independently (even though they are not independent). Incidentally, the non-independence of the eigenvalues results in physical phenomena where a random matrix description works, such as level repulsion. After a little bit of work and using the machinery of orthogonal polynomials, this results in
\begin{equation}
  \rho_{\mathrm{GUE}}(\lambda) = \frac{1}{N \sqrt{2\pi}} e^{-\lambda^2/2} \sum_{k = 0}^{N-1} \frac{1}{2^k k!} H_k\left(\frac{\lambda}{\sqrt{2}}\right)^2
\end{equation}
where $H_k$ denotes the $k^\mathrm{th}$ Hermite polynomial. Further, the asymptotic formulae for the Hermite polynomials can be used to prove the Wigner semicircle law (that holds as $N \to \infty$).

The corresponding formulae for traceless GUE can also be obtained with some work, but as of now, we do not have a closed-form distribution of eigenvalues. It is, however, interesting to note a relationship between the eigenvalue distribution of GUE and traceless GUE (see, e.g., \cite{1999math......4042T}) that schematically looks like
\begin{equation}
  \rho_{\mathrm{GUE}}(\lambda) = \sqrt{\frac{N}{\pi}} \int_{\mathbb{R}} \text{d} t e^{-N t^2} \rho_{\mathrm{tGUE}}(\lambda - t)
\end{equation}

For an accessible introduction to the theory of random matrices, see \cite{Livan_2018}. 



\chapter{Matrix Spherical Harmonics}
\label{app:matrix-harmonics}

In trying to understand a theory of matrices, it is useful to think of a configuration of them (traceless, Hermitian, in our case) as functions on $\B{S}^2$. In particular, these functions can be restricted to square integrable $L^2(\B{S}^2)$. Another way to approach this is by considering the matrices as a truncation of the set of all square-integrable functions on $\B{S}^2$. Thus, one has a map $\mu$.
  \begin{align}
    \mu \colon \mathfrak{u}(N) &\to L^2(\B{S}^2) \cr
    X &\mapsto \mu(X)
  \end{align}
  This map must respect the Hilbert space structure on the two sides. In addition, the two sides have a natural action of $\mathrm{SU}(2)$ defined on them. We can require our map to be equivariant with respect to this action. In total, we can list three requirements for $\mu$
  \begin{enumerate}
    \item Linearity
      \begin{equation}
        \mu(\alpha X + \beta Y) \; = \; \alpha \, \mu(X) + \beta \, \mu(Y)
      \end{equation}
    \item Preserve inner product
      \begin{equation}
        \underbrace{\frac{1}{N} \Tr X^\dagger Y}_{\substack{\text{Hilbert-Schmidt inner} \\ \text{product on }\mathfrak{u}(N)}} \; = \; \underbrace{\int_{\B{S}^2} \text{d}\Omega \, \mu(X)^* \mu(Y)}_{L^2\text{ inner product}}
      \end{equation}
    \item Preserve the action of $\mathrm{SU}(2)$\\
      $\mathrm{SU}(2)$ acts on $\mathfrak{u}(N)$ by it's adjoint action. On $\B{S}^2$, $\mathrm{SU}(2)$ acts by rotation which is pushed over to $L^2(\B{S}^2)$
      \begin{align}
        \mathfrak{u}(N) \ni X \; &\xrightarrow{U \, \in \, \mathrm{SU}(2)} \; U^\dagger X U \cr
        L^2(\B{S}^2) \ni f(\theta, \phi) \; &\xrightarrow{U \, \in \, \mathrm{SU}(2)} \; f(U^{-1} (\theta, \phi)) \cr
        U^\dagger X U \; &\xrightarrow{\quad\mu\quad} \; \mu(X)(U^{-1} (\theta, \phi))
      \end{align}
      Or, at the level of $\mathrm{SU}(2)$ generators $\mathfrak{g} = \alpha_i J^i$
      \begin{equation}
        \comm{J^i}{X} \; \xrightarrow{\quad\mu\quad} \; \xi^{(i)}(\mu(X))
      \end{equation}
      where $\xi^{(i)}$ are the Killing fields associated to $J^i$. Explicitly,
      \begin{equation}
        \xi^{(i)} = -i \epsilon_{i j k} x^j \frac{\partial}{\partial x^k} \quad \text{with} \quad x^k = (\sin \theta \cos \phi, \sin \theta \sin \phi, \cos \theta)
      \end{equation}
  \end{enumerate}

  To explicitly construct such a map, it is enough to define it on a basis $\{\B{Z}_a\}$ of $\mathfrak{u}(N)$. However, we would also like to make use of the rotational covariance, which is more manifest on the right-hand side of this map: We know that spherical harmonics $Y^{j}_{m}$ ($j = 0, 1, \ldots$ and $m = -j, -j+1, \ldots, j$) give a convenient orthonormal basis of $L^2(\B{S}^2)$ which also transforms nicely under $\mathfrak{su}(2)$. That is, every $j$-multiplet of $Y^{j}_{m}$ furnishes the usual spin-$j$ representation of $\mathfrak{su}(2)$. For example,
  \begin{align}
    \xi^{3}Y^{j}_{m} &= m Y^{j}_{m} \cr
    \xi^{i} \xi^{i} Y^{j}_{m} &= j(j+1) Y^{j}_{m} \cr
    \frac{1}{4\pi} \int_{\B{S}^2} \text{d}\Omega \, {Y^{j_1}_{m_1}}^* {Y^{j_2}_{m_2}} &= \delta^{j_1}_{j_2} \delta^{m_1}_{m_2} 
  \end{align}

  Thus, if we can define the inverse map $\mu^{-1}$ on these $Y^{j}_{m}$, we'd be done. It is clear, though, that the map $\mu$ as it is defined above fails to be surjective: $L^2(\B{S}^2)$ is an infinite dimensional vector space whereas $\mathfrak{u}(N)$ is $N^2$ dimensional. However, we can still define $\mu$ on a truncated set of $Y^{j}_{m}$. The truncation is easy to derive: each $j$-multiplet is $(2j+1)$ dimensional. Suppose we truncate at $j = j_{\text{max}}$: i.e., we only take $j \in \{0, 1, \ldots, j_{\text{max}}\}$, then it must be that
  \begin{equation}
    \sum_{j=0}^{j_{\text{max}}} (2j+1) = (j_{\text{max}}+1)^2 = N^2 \quad \implies \quad j_{\text{max}} = N-1
  \end{equation}
  Therefore, we must define the so-called \textit{matrix harmonics} $Z^{j}_{m} \coloneqq \mu^{-1}(Y^j_m)$ ($j = 0, 1, \ldots, N-1$ and $m = -j, -j+1, \ldots, j$)\\

  Fortunately, we do have some special $\mathfrak{u}(N)$ matrices that can help: the spin-$\frac{N-1}{2}$ dimensional representation of $\mathfrak{su}(2)$. For the spin-$j$ irrep
  \begin{equation}
    J^{3}_{s s'} \coloneqq \mel{j\,s'}{J^3}{j\,s} = s \, \delta_{s s'}
  \end{equation}
  And $J^{1}$, $J^2$ are given in terms of $J^{\pm} \coloneqq J^1 \pm i J^2$
  \begin{align}
    J^\pm_{s s'} &= C^{\pm}_{j s} \, \delta_{s \pm 1, s'} \cr
    C^{\pm}_{j s} &= \sqrt{(j \mp s)(j \pm s + 1)}
  \end{align}
  The $Z^j_m$ can be found by starting at the lowest "rung of the ladder," $Z^j_{-j}$ and successively applying $J^+$
  \begin{equation}
    Z^j_m = \frac{1}{C^{+}_{j \, m-1}} \comm{J^+}{Z^j_{m-1}}
  \end{equation}
  Note that $\comm{J^-}{Z^j_{-j}} = 0$ so a plausible choice could be $Z^j_{-j} = f(J^-)$. Also, $\comm{J^+}{{(J^-)}^m} \sim {(J^-)}^{m-1}$, so we take
  \begin{equation}
    Z^j_{-j} = c \, {(J^-)}^{j}
  \end{equation}
  where $c$ is found by requiring orthonormality
  \begin{equation}
    \frac{1}{N} \Tr ({Z^{j_1}_{m_1}})^{\dagger} Z^{j_2}_{m_2} = \delta^{j_1}_{j_2} \delta^{m_1}_{m_2}
  \end{equation}